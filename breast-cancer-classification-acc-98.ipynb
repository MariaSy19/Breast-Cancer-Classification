{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tqdm import tqdm\nimport shutil\n\n# Paths and settings\nINPUT_DIR = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nOUTPUT_DIR = '/kaggle/working/preprocessed_balanced_dataset'\nTARGET_SIZE = (224, 224)\nTARGET_COUNT = 891\nCLASSES = ['benign', 'malignant', 'normal']\n\n# Clean previous outputs\nif os.path.exists(OUTPUT_DIR):\n    shutil.rmtree(OUTPUT_DIR)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# SEGMENTATION FUNCTION \ndef apply_kmeans_segmentation(image, k=2):\n    Z = image.reshape((-1, 1)).astype(np.float32)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n    _, labels, centers = cv2.kmeans(Z, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    segmented_img = centers[labels.flatten()].reshape(image.shape)\n    segmented_img = np.uint8(segmented_img)\n    return segmented_img\n\n#PREPROCESSING FUNCTION \ndef preprocess_image(image_path):\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    image = cv2.resize(image, TARGET_SIZE)\n    image = cv2.equalizeHist(image)\n    image = cv2.GaussianBlur(image, (5, 5), 0)\n    image = cv2.bilateralFilter(image, 9, 75, 75)\n    image = apply_kmeans_segmentation(image) \n    return image\n\n#SAVE IMAGES FUNCTION  \ndef save_preprocessed_images(class_name):\n    input_path = os.path.join(INPUT_DIR, class_name)\n    output_path = os.path.join(OUTPUT_DIR, class_name)\n    os.makedirs(output_path, exist_ok=True)\n\n    image_files = [f for f in os.listdir(input_path) if f.endswith('.png') and 'mask' not in f]\n    count = 0\n\n    for i, filename in enumerate(tqdm(image_files, desc=f\"Preprocessing {class_name}\")):\n        if count >= TARGET_COUNT:\n            break\n        img_path = os.path.join(input_path, filename)\n        processed = preprocess_image(img_path)\n        cv2.imwrite(os.path.join(output_path, f\"{class_name}_{i}.png\"), processed)\n        count += 1\n\n    return output_path, count\n\n#AUGMENTATION FUNCTION \ndef augment_class_images(class_name, current_count):\n    if current_count >= TARGET_COUNT:\n        return\n\n    datagen = ImageDataGenerator(\n        rotation_range=15,\n        zoom_range=0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest'\n    )\n\n    class_dir = os.path.join(OUTPUT_DIR, class_name)\n    images = [f for f in os.listdir(class_dir) if f.endswith('.png')]\n    img_idx = 0\n\n    while current_count < TARGET_COUNT:\n        img_path = os.path.join(class_dir, images[img_idx % len(images)])\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = img.reshape((1, img.shape[0], img.shape[1], 1))\n        for batch in datagen.flow(img, batch_size=1, save_to_dir=class_dir,\n                                  save_prefix='aug', save_format='png'):\n            current_count += 1\n            if current_count >= TARGET_COUNT:\n                break\n        img_idx += 1\n\n#MAIN PREPROCESS + AUGMENT\nfor cls in CLASSES:\n    out_dir, count = save_preprocessed_images(cls)\n    print(f\"{cls} class before augmentation: {count} images\")\n    augment_class_images(cls, count)\n    final_count = len([f for f in os.listdir(out_dir) if f.endswith('.png')])\n    print(f\"{cls} class after augmentation: {final_count} images\")\n\nprint(\"\\n Processing and balancing completed.\")\n\n#MODEL DEFINITION \nIMG_SIZE = 224\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(3, activation='softmax')  \n])\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(\"\\n CNN model defined and compiled.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:30:47.966101Z","iopub.execute_input":"2025-05-17T16:30:47.966439Z","iopub.status.idle":"2025-05-17T16:32:23.944425Z","shell.execute_reply.started":"2025-05-17T16:30:47.966413Z","shell.execute_reply":"2025-05-17T16:32:23.943642Z"}},"outputs":[{"name":"stderr","text":"2025-05-17 16:30:50.651224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747499450.939146      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747499451.023972      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nPreprocessing benign: 100%|██████████| 437/437 [00:37<00:00, 11.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"benign class before augmentation: 437 images\nbenign class after augmentation: 884 images\n","output_type":"stream"},{"name":"stderr","text":"Preprocessing malignant: 100%|██████████| 210/210 [00:17<00:00, 11.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"malignant class before augmentation: 210 images\nmalignant class after augmentation: 864 images\n","output_type":"stream"},{"name":"stderr","text":"Preprocessing normal: 100%|██████████| 133/133 [00:11<00:00, 11.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"normal class before augmentation: 133 images\nnormal class after augmentation: 864 images\n\n✅ Processing and balancing completed.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n2025-05-17 16:32:23.620101: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"\n CNN model defined and compiled.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\ntrain_generator = datagen.flow_from_directory(\n    OUTPUT_DIR,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    color_mode='grayscale',\n    batch_size=32,\n    class_mode='categorical',\n    subset='training'\n)\n\nval_generator = datagen.flow_from_directory(\n    OUTPUT_DIR,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    color_mode='grayscale',\n    batch_size=32,\n    class_mode='categorical',\n    subset='validation'\n)\n\nhistory = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator\n)\n\nloss, acc = model.evaluate(val_generator)\nprint(f\"\\nFinal Accuracy: {acc:.4f}, Loss: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T16:32:55.431153Z","iopub.execute_input":"2025-05-17T16:32:55.432272Z","iopub.status.idle":"2025-05-17T16:52:32.586320Z","shell.execute_reply.started":"2025-05-17T16:32:55.432244Z","shell.execute_reply":"2025-05-17T16:52:32.585398Z"}},"outputs":[{"name":"stdout","text":"Found 2092 images belonging to 3 classes.\nFound 520 images belonging to 3 classes.\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.6268 - loss: 2.4947 - val_accuracy: 0.9865 - val_loss: 0.0585\nEpoch 2/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - accuracy: 0.8844 - loss: 0.2795 - val_accuracy: 0.9942 - val_loss: 0.0268\nEpoch 3/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.9585 - loss: 0.1203 - val_accuracy: 0.9942 - val_loss: 0.0246\nEpoch 4/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9955 - loss: 0.0233 - val_accuracy: 0.9942 - val_loss: 0.0411\nEpoch 5/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.9971 - loss: 0.0121 - val_accuracy: 0.9942 - val_loss: 0.0291\nEpoch 7/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9964 - loss: 0.0147 - val_accuracy: 0.9962 - val_loss: 0.0192\nEpoch 8/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9991 - loss: 0.0073 - val_accuracy: 0.9923 - val_loss: 0.0413\nEpoch 9/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 0.9923 - val_loss: 0.0285\nEpoch 10/10\n\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.9996 - loss: 0.0022 - val_accuracy: 0.9865 - val_loss: 0.0516\n\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 389ms/step - accuracy: 0.9802 - loss: 0.0721\n\nFinal Accuracy: 0.9865, Loss: 0.0516\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}